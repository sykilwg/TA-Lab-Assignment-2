{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5737c368-cc41-4d3e-b180-f1a8dcc2fee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nsbih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nsbih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Naive Bayes =====\n",
      "[[ 2923    12  8351]\n",
      " [  311    21  5553]\n",
      " [  284     3 61277]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.26      0.39     11286\n",
      "     neutral       0.58      0.00      0.01      5885\n",
      "    positive       0.82      1.00      0.90     61564\n",
      "\n",
      "    accuracy                           0.82     78735\n",
      "   macro avg       0.74      0.42      0.43     78735\n",
      "weighted avg       0.80      0.82      0.76     78735\n",
      "\n",
      "\n",
      "===== Logistic Regression =====\n",
      "[[ 7275   446  3565]\n",
      " [ 1343   863  3679]\n",
      " [ 1506   564 59494]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.64      0.68     11286\n",
      "     neutral       0.46      0.15      0.22      5885\n",
      "    positive       0.89      0.97      0.93     61564\n",
      "\n",
      "    accuracy                           0.86     78735\n",
      "   macro avg       0.69      0.59      0.61     78735\n",
      "weighted avg       0.83      0.86      0.84     78735\n",
      "\n",
      "\n",
      "===== Linear SVC =====\n",
      "[[ 7352   203  3731]\n",
      " [ 1468   510  3907]\n",
      " [ 1527   256 59781]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.65      0.68     11286\n",
      "     neutral       0.53      0.09      0.15      5885\n",
      "    positive       0.89      0.97      0.93     61564\n",
      "\n",
      "    accuracy                           0.86     78735\n",
      "   macro avg       0.71      0.57      0.59     78735\n",
      "weighted avg       0.83      0.86      0.83     78735\n",
      "\n",
      "\n",
      "===== Random Forest =====\n",
      "[[ 4142    40  7104]\n",
      " [  511   146  5228]\n",
      " [  451    41 61072]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.37      0.51     11286\n",
      "     neutral       0.64      0.02      0.05      5885\n",
      "    positive       0.83      0.99      0.90     61564\n",
      "\n",
      "    accuracy                           0.83     78735\n",
      "   macro avg       0.76      0.46      0.49     78735\n",
      "weighted avg       0.81      0.83      0.78     78735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\nsbih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VADER Lexicon-Based Sentiment Accuracy: 0.7968 =====\n",
      "\n",
      "Name: NUR SYAKILA BINTI IZWAN HADI WONG\n",
      "Student ID: IS01082922\n",
      "\n",
      "Discussion:\n",
      "Lexicon-based sentiment analysis using VADER is simple and doesn't require training data,\n",
      "but it struggles with neutral tones and complex expressions. Among ML models, Linear SVC\n",
      "and Logistic Regression performed best, especially in handling large feature spaces like TF-IDF.\n",
      "They can capture subtleties in review texts, though they require more processing and training time.\n",
      "\n",
      "In conclusion, ML-based models provide better performance and flexibility for sentiment analysis\n",
      "on this Amazon review dataset compared to lexicon-based approaches.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "df = df[['Score', 'Text']]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "def map_sentiment(score):\n",
    "    if score >= 4:\n",
    "        return 'positive'\n",
    "    elif score == 3:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "df['Sentiment'] = df['Score'].apply(map_sentiment)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    words = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "df['CleanText'] = df['Text'].apply(clean_text)\n",
    "df[['CleanText', 'Sentiment']].to_csv('processed_reviews.csv', index=False)\n",
    "\n",
    "# Step 2: Feature Extraction\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['CleanText'])\n",
    "y = df['Sentiment']\n",
    "\n",
    "# Step 3: Train/Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Machine Learning Models\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Linear SVC': LinearSVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 5: Lexicon-Based Sentiment (VADER)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment(text):\n",
    "    score = sid.polarity_scores(text)['compound']\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df_sample = df.sample(5000, random_state=42)\n",
    "df_sample['VADER_Predicted'] = df_sample['Text'].apply(vader_sentiment)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "vader_accuracy = accuracy_score(df_sample['Sentiment'], df_sample['VADER_Predicted'])\n",
    "print(f\"\\n===== VADER Lexicon-Based Sentiment Accuracy: {vader_accuracy:.4f} =====\")\n",
    "\n",
    "print(\"\"\"\n",
    "Name: NUR SYAKILA BINTI IZWAN HADI WONG\n",
    "Student ID: IS01082922\n",
    "\n",
    "Discussion:\n",
    "Lexicon-based sentiment analysis using VADER is simple and doesn't require training data,\n",
    "but it struggles with neutral tones and complex expressions. Among ML models, Linear SVC\n",
    "and Logistic Regression performed best, especially in handling large feature spaces like TF-IDF.\n",
    "They can capture subtleties in review texts, though they require more processing and training time.\n",
    "\n",
    "In conclusion, ML-based models provide better performance and flexibility for sentiment analysis\n",
    "on this Amazon review dataset compared to lexicon-based approaches.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8b1129-d088-418b-b492-75d51fc94986",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.read_csv('processed_reviews.csv')\n",
    "\n",
    "midpoint = len(df_processed) // 2\n",
    "df_part1 = df_processed.iloc[:midpoint]\n",
    "df_part2 = df_processed.iloc[midpoint:]\n",
    "\n",
    "df_part1.to_csv('processed_reviews_part1.csv', index=False)\n",
    "df_part2.to_csv('processed_reviews_part2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52b9ef-c4ec-4406-bc99-225667a8022d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
